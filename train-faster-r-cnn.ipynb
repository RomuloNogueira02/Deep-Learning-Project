{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8470843,"sourceType":"datasetVersion","datasetId":5051007}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\n\nimport torchvision\nfrom torchvision.transforms import v2, PILToTensor\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\n\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom torch.utils.data import DataLoader\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nimport xml.etree.ElementTree as ET\nfrom PIL import Image\n\n","metadata":{"_uuid":"52c2a5c6-5846-4ad3-95a4-3913498a9384","_cell_guid":"25914755-370a-421d-b193-6f7f722c2f7a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:09.657354Z","iopub.execute_input":"2024-05-20T22:23:09.657670Z","iopub.status.idle":"2024-05-20T22:23:21.170691Z","shell.execute_reply.started":"2024-05-20T22:23:09.657632Z","shell.execute_reply":"2024-05-20T22:23:21.169916Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def show_image(image):\n    plt.imshow(image.permute(1,2,0))\n    plt.axis('off')\n    plt.show()\n\n    \ndef show_image_with_box(image,boxes):\n\n    image_draw = image.permute(1,2,0).clone().cpu().numpy()\n    boxes_np = [box.tolist() for box in boxes]\n\n    for box in boxes_np:\n        x1, y1, x2, y2 = box\n        cv2.rectangle(image_draw, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 1)\n    \n    plt.imshow(image_draw)\n    plt.axis('off')\n    plt.show()\n\ndef get_images_labeled(labels_path):\n    return os.listdir(labels_path)\n\ndef change_image_to_label(image_path):\n    new_path = image_path.replace('images','labels')\n    if 'jpg' in new_path:\n        new_path = new_path.replace('jpg','xml')\n    elif 'png' in new_path:\n        new_path = new_path.replace('png','xml')\n    \n    return new_path\n\ndef getBoxes(xml_path):\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n\n    boxes = []\n\n    for elem in root:\n        if elem.tag == 'object':\n            for obj_elem in elem:\n                if obj_elem.tag == 'bndbox':\n                    line = []\n                    for bbox_elem in obj_elem:\n                        value = float(bbox_elem.text)\n                        line.append(value)\n                    boxes.append(line)\n    return boxes\n\ndef getLabels(xml_path):\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n\n    labels = []\n\n    for elem in root:\n        if elem.tag == 'object':\n            for obj_elem in elem:\n                if obj_elem.tag == 'name':\n                    labels.append(1)\n    return labels\n\ndef plot(imgs, row_title=None, **imshow_kwargs):\n    if not isinstance(imgs[0], list):\n        # Make a 2d grid even if there's just 1 row\n        imgs = [imgs]\n\n    num_rows = len(imgs)\n    num_cols = len(imgs[0])\n    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n    for row_idx, row in enumerate(imgs):\n        for col_idx, img in enumerate(row):\n            boxes = None\n            masks = None\n            if isinstance(img, tuple):\n                img, target = img\n                if isinstance(target, dict):\n                    boxes = target.get(\"boxes\")\n                    masks = target.get(\"masks\")\n                elif isinstance(target, tv_tensors.BoundingBoxes):\n                    boxes = target\n                else:\n                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n            img = F.to_image(img)\n            if img.dtype.is_floating_point and img.min() < 0:\n                # Poor man's re-normalization for the colors to be OK-ish. This\n                # is useful for images coming out of Normalize()\n                img -= img.min()\n                img /= img.max()\n\n            img = F.to_dtype(img, torch.uint8, scale=True)\n            if boxes is not None:\n                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n            if masks is not None:\n                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n\n            ax = axs[row_idx, col_idx]\n            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n    if row_title is not None:\n        for row_idx in range(num_rows):\n            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T22:23:21.172155Z","iopub.execute_input":"2024-05-20T22:23:21.172530Z","iopub.status.idle":"2024-05-20T22:23:21.194101Z","shell.execute_reply.started":"2024-05-20T22:23:21.172505Z","shell.execute_reply":"2024-05-20T22:23:21.193143Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class PoolDatasetV2(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n\n        self.labeled_images = get_images_labeled(self.root_dir.replace('images','labels'))\n        self.retrieve_data()\n\n    def retrieve_data(self):\n        self.images = []\n        self.boxes = []\n        self.labels = []\n        \n        for file in os.listdir(self.root_dir):\n            \n            full_image_path = self.root_dir + \"/\" + file\n            label_name = change_image_to_label(file)\n\n\n            # Deal with the image\n            img = Image.open(full_image_path).convert('RGB')\n\n            # Transform to tensor\n            if \"PIL\" in str(type(img)):\n                img = PILToTensor()(img)\n            else:\n                img = torch.Tensor(img)\n\n            self.images.append(img)\n\n\n            # Deal with the label\n            if label_name in self.labeled_images:\n                boxes = getBoxes(self.root_dir.replace('images','labels') + \"/\" + label_name)\n                labels = getLabels(self.root_dir.replace('images','labels') + \"/\" + label_name)\n                self.labels.append(labels)\n                #self.boxes.append([torch.Tensor(box) for box in boxes])\n                self.boxes.append(boxes)\n\n            else:\n                self.boxes.append([torch.Tensor([0.0, 0.0, 224.0, 224.0])])\n                self.labels.append([0])\n\n    def __len__(self):\n        return len(self.images)\n    \n\n    def __getitem__(self, idx):\n        target = {}\n        img = self.images[idx]\n        boxes = self.boxes[idx]\n        labels = self.labels[idx]\n\n        old_width = img.shape[1]\n\n        if self.transform:\n            img = torch.Tensor(self.transform(img))\n            \n            factor = old_width / img.shape[1]\n\n            if len(boxes) > 0:\n                #print(boxes)\n                boxes = list(map(lambda l: [v / factor for v in l], boxes))\n                #print(boxes)\n                boxes = torch.Tensor(boxes)\n                \n                #max_boxes = max(len(boxes), 1)  # Ensure at least 1 box\n                #pad_boxes = F.pad(torch.stack(boxes), (0, 0, 0, max_boxes - len(boxes)), value=-1)\n                #boxes = pad_boxes.unbind(0)\n                labels = torch.IntTensor(labels)\n        else:\n            boxes = torch.Tensor(boxes)\n            labels = torch.IntTensor(labels, dtype=torch.int64)\n        \n        target['boxes'] = boxes\n        target['labels'] = labels\n\n        return img, target\n    \n    def split_Data(self, n_test=0.33):\n        test_size = round(n_test * len(self.images))\n        train_size = len(self.images) - test_size\n\n        return random_split(self, [train_size, test_size])\n    \n    def collate(self,batch):\n        images = list()\n        boxes = list()\n\n        for b in batch:\n            images.append(b[0])\n            boxes.append(b[1])\n        \n        images = torch.stack(images, dim=0)\n        return images,boxes","metadata":{"execution":{"iopub.status.busy":"2024-05-20T22:23:21.195295Z","iopub.execute_input":"2024-05-20T22:23:21.195645Z","iopub.status.idle":"2024-05-20T22:23:21.215651Z","shell.execute_reply.started":"2024-05-20T22:23:21.195615Z","shell.execute_reply":"2024-05-20T22:23:21.214855Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"def show_image_with_box(image,boxes):\n\n    image_draw = image.permute(1,2,0).clone().cpu().numpy()\n    boxes_np = [box.tolist() for box in boxes]\n    for box in boxes_np:\n        x1, y1, x2, y2 = box\n        cv2.rectangle(image_draw, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 1)\n    \n    plt.imshow(image_draw)\n    plt.axis('off')\n    plt.show()","metadata":{"_uuid":"9d4257ff-ab87-4ac5-9941-f15627f5d99f","_cell_guid":"3adf72f3-152a-4450-ae80-25b978835e8e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:21.218180Z","iopub.execute_input":"2024-05-20T22:23:21.218707Z","iopub.status.idle":"2024-05-20T22:23:21.231691Z","shell.execute_reply.started":"2024-05-20T22:23:21.218679Z","shell.execute_reply":"2024-05-20T22:23:21.230828Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"transforms = v2.Compose([\n    v2.Resize((224, 224)),\n    v2.ToImage(),\n    #v2.RandomHorizontalFlip(p=1),\n    v2.ToDtype(torch.float32, scale=True),\n    #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224,Â 0.225])\n])\n\nROOT_DIR = \"/kaggle/input/dataset/subset/images\"","metadata":{"_uuid":"882e9b34-cbb0-40d8-b0c6-b6689042de8e","_cell_guid":"dacbe3b6-d814-456f-8847-fda47236a880","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:21.232820Z","iopub.execute_input":"2024-05-20T22:23:21.233106Z","iopub.status.idle":"2024-05-20T22:23:21.241289Z","shell.execute_reply.started":"2024-05-20T22:23:21.233083Z","shell.execute_reply":"2024-05-20T22:23:21.240562Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ds = PoolDatasetV2(ROOT_DIR,transforms)","metadata":{"_uuid":"2e5aeff6-e3ed-44a4-aba6-fa213d1ec339","_cell_guid":"ed29dd7d-7cc4-485f-86ef-5cfd5bbe688c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:21.242426Z","iopub.execute_input":"2024-05-20T22:23:21.242710Z","iopub.status.idle":"2024-05-20T22:23:22.024315Z","shell.execute_reply.started":"2024-05-20T22:23:21.242688Z","shell.execute_reply":"2024-05-20T22:23:22.023478Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train, test = ds.split_Data(0.33)\nlen(train)","metadata":{"_uuid":"6cdb9675-350f-4a85-94f9-698937916d10","_cell_guid":"a5fdc9c7-7d4f-47ad-a28c-e2ea3c396a3f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:22.025406Z","iopub.execute_input":"2024-05-20T22:23:22.025690Z","iopub.status.idle":"2024-05-20T22:23:22.062293Z","shell.execute_reply.started":"2024-05-20T22:23:22.025666Z","shell.execute_reply":"2024-05-20T22:23:22.061455Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"34"},"metadata":{}}]},{"cell_type":"code","source":"train[1]","metadata":{"_uuid":"6a150d6f-e31a-4624-80c4-af888a01e618","_cell_guid":"7a83a7bf-a05e-4eff-92f9-a8e57ab55df2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:22.063417Z","iopub.execute_input":"2024-05-20T22:23:22.063889Z","iopub.status.idle":"2024-05-20T22:23:22.186026Z","shell.execute_reply.started":"2024-05-20T22:23:22.063845Z","shell.execute_reply":"2024-05-20T22:23:22.185200Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(tensor([[[0.3255, 0.2235, 0.3412,  ..., 0.0706, 0.0863, 0.0941],\n          [0.3294, 0.2667, 0.4235,  ..., 0.0784, 0.0902, 0.0784],\n          [0.3333, 0.3686, 0.3451,  ..., 0.0667, 0.0824, 0.0549],\n          ...,\n          [0.0627, 0.0627, 0.0588,  ..., 0.2784, 0.2431, 0.1882],\n          [0.0627, 0.0627, 0.0667,  ..., 0.2784, 0.2745, 0.2471],\n          [0.0471, 0.0549, 0.0706,  ..., 0.2588, 0.3059, 0.3137]],\n \n         [[0.3294, 0.2275, 0.3490,  ..., 0.0980, 0.1059, 0.1137],\n          [0.3333, 0.2745, 0.4314,  ..., 0.1059, 0.1176, 0.1059],\n          [0.3412, 0.3765, 0.3529,  ..., 0.0941, 0.1098, 0.0902],\n          ...,\n          [0.1059, 0.1059, 0.1020,  ..., 0.2824, 0.2471, 0.1922],\n          [0.1059, 0.1059, 0.1020,  ..., 0.2784, 0.2706, 0.2431],\n          [0.0902, 0.0980, 0.1059,  ..., 0.2588, 0.2980, 0.3059]],\n \n         [[0.2667, 0.1647, 0.2980,  ..., 0.1294, 0.1294, 0.1373],\n          [0.2706, 0.2196, 0.3804,  ..., 0.1373, 0.1490, 0.1294],\n          [0.2863, 0.3216, 0.3020,  ..., 0.1333, 0.1490, 0.1176],\n          ...,\n          [0.1137, 0.1137, 0.1255,  ..., 0.2902, 0.2627, 0.2078],\n          [0.1137, 0.1137, 0.1294,  ..., 0.2863, 0.2902, 0.2627],\n          [0.0980, 0.1059, 0.1333,  ..., 0.2667, 0.3176, 0.3255]]]),\n {'boxes': tensor([[122.9000,   0.0000, 165.1300,  15.3000]]),\n  'labels': tensor([1], dtype=torch.int32)})"},"metadata":{}}]},{"cell_type":"code","source":"# def collate_fn(batch):\n#     images = [item[0] for item in batch]\n#     boxes = [item[1] for item in batch]\n\n#     max_size_labels = max([len(box) for box in boxes])\n\n#     for box in boxes:\n#         if len(box) < max_size_labels:\n#             toadd = torch.Tensor([0.0, 0.0, 0.0, 0.0])\n#             box.extend([toadd for i in range(max_size_labels - len(box))])\n\n#     return images, boxes\n\ndef collate_fn(batch):\n    return batch","metadata":{"_uuid":"59d7c222-b4f6-4f43-a229-20284b75881b","_cell_guid":"ecc73910-e29c-44bf-8fc4-af9f95c75dd3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:22.187281Z","iopub.execute_input":"2024-05-20T22:23:22.187563Z","iopub.status.idle":"2024-05-20T22:23:22.192018Z","shell.execute_reply.started":"2024-05-20T22:23:22.187539Z","shell.execute_reply":"2024-05-20T22:23:22.191134Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train, batch_size=16, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)\ntest_loader = DataLoader(test, batch_size=16, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)","metadata":{"_uuid":"fc19e7a9-25a5-4e0e-a190-4dbfbde04e04","_cell_guid":"d171da17-f3e2-46b3-830a-749a50359b1d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:23:22.195274Z","iopub.execute_input":"2024-05-20T22:23:22.195544Z","iopub.status.idle":"2024-05-20T22:23:22.257113Z","shell.execute_reply.started":"2024-05-20T22:23:22.195522Z","shell.execute_reply":"2024-05-20T22:23:22.256220Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"01f53874-7bfd-4bf6-9636-dbf270c87ac3","_cell_guid":"0cd90293-8916-4ba1-b024-ff10676b311b","trusted":true}},{"cell_type":"code","source":"model = fasterrcnn_resnet50_fpn(pretrained=False)\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)","metadata":{"_uuid":"83a12291-333f-40db-98b7-9dc3ae1ad2df","_cell_guid":"3d620e29-5b20-4f75-a7b2-3036c2c6aa19","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:38:57.378033Z","iopub.execute_input":"2024-05-20T22:38:57.378367Z","iopub.status.idle":"2024-05-20T22:38:58.136337Z","shell.execute_reply.started":"2024-05-20T22:38:57.378342Z","shell.execute_reply":"2024-05-20T22:38:58.135546Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"_uuid":"fe37a162-3b70-4179-8167-b2f5037bc33b","_cell_guid":"ec01eed7-0c47-4e84-83e3-a5b710c7c9b5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:38:58.834628Z","iopub.execute_input":"2024-05-20T22:38:58.834992Z","iopub.status.idle":"2024-05-20T22:38:58.839523Z","shell.execute_reply.started":"2024-05-20T22:38:58.834962Z","shell.execute_reply":"2024-05-20T22:38:58.838642Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nnum_epochs = 10","metadata":{"_uuid":"20ff09a1-ed57-478e-bdd1-9c8d0435336c","_cell_guid":"4ee7bd93-a166-4c14-9fce-6a0deb5fc771","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:38:59.891036Z","iopub.execute_input":"2024-05-20T22:38:59.891425Z","iopub.status.idle":"2024-05-20T22:38:59.897406Z","shell.execute_reply.started":"2024-05-20T22:38:59.891397Z","shell.execute_reply":"2024-05-20T22:38:59.896447Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    start = time.time()\n    for data in train_loader:\n        # print(data)\n        imgs = []\n        targets = []\n        for d in data:\n            imgs.append(d[0].to(device))\n            targ = {}\n            targ['boxes'] = d[1]['boxes'].to(device)\n            targ['labels'] = d[1]['labels'].to(torch.int64).to(device)\n            targets.append(targ)\n\n        loss_dict = model(imgs, targets)\n        loss = sum(loss for loss in loss_dict.values())\n        epoch_loss += loss.cpu().detach().numpy()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    total_time = time.time() - start\n    print(f'Epoch: {epoch}, Loss: {epoch_loss} -> {round(total_time)}s')","metadata":{"_uuid":"dd7aa493-e325-4257-b6c7-8a46fd2cd362","_cell_guid":"a0ac4521-3705-47fd-9165-fcb482dc3f31","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:39:01.379193Z","iopub.execute_input":"2024-05-20T22:39:01.379548Z","iopub.status.idle":"2024-05-20T22:39:59.412463Z","shell.execute_reply.started":"2024-05-20T22:39:01.379521Z","shell.execute_reply":"2024-05-20T22:39:59.411530Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Epoch: 0, Loss: 2.8054462671279907 -> 6s\nEpoch: 1, Loss: 1.719754934310913 -> 6s\nEpoch: 2, Loss: 1.315606951713562 -> 6s\nEpoch: 3, Loss: 1.0491372048854828 -> 6s\nEpoch: 4, Loss: 1.1060037016868591 -> 6s\nEpoch: 5, Loss: 0.7835223078727722 -> 6s\nEpoch: 6, Loss: 1.0069560110569 -> 6s\nEpoch: 7, Loss: 0.7276667952537537 -> 6s\nEpoch: 8, Loss: 0.7058354914188385 -> 6s\nEpoch: 9, Loss: 0.7192432731389999 -> 6s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"-----------------","metadata":{}},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_iou(box1, box2):\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    \n    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n    area_box1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n    area_box2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n    \n    union = area_box1 + area_box2 - intersection\n    \n    iou = intersection / union\n    return iou\n\ndef evaluate_detections(pred_boxes, gt_boxes, iou_threshold=0.5):\n    TP, FP, FN = 0, 0, 0\n    matched_gt_boxes = set()\n    \n    for pred_box in pred_boxes:\n        matched = False\n        for gt_box in gt_boxes:\n            if compute_iou(pred_box, gt_box) >= iou_threshold:\n                if gt_box not in matched_gt_boxes:\n                    TP += 1\n                    matched_gt_boxes.add(gt_box)\n                    matched = True\n                    break\n        if not matched:\n            FP += 1\n    \n    FN = len(gt_boxes) - len(matched_gt_boxes)\n    return TP, FP, FN\n\ndef calculate_metrics(TP, FP, FN):\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    return precision, recall, f1_score\n\ndef calculate_accuracy(TP, FP, FN):\n    total_predictions = TP + FP\n    total_ground_truths = TP + FN\n    accuracy = TP / (total_predictions + total_ground_truths - TP) if (total_predictions + total_ground_truths - TP) > 0 else 0\n    return accuracy\n\ndef evaluate_model(model, dataloader, device, iou_threshold=0.5, min_confidence = 0.7):\n    model.eval()\n    TP, FP, FN = 0, 0, 0\n\n    with torch.no_grad():\n        for data in dataloader:\n            imgs = []\n            targets = []\n            for d in data:\n                imgs.append(d[0].to(device))\n                targ = {}\n                targ['boxes'] = d[1]['boxes']\n                targ['labels'] = d[1]['labels']\n                targets.append(targ)\n\n            outputs = model(imgs)\n                       \n            \n            for i in range(len(imgs)):\n                \n                scores = outputs[i]['scores']\n                high_score_indices = scores >= min_confidence\n                \n                pred_boxes = outputs[i]['boxes'][high_score_indices]\n                gt_boxes = targets[i]['boxes']\n\n                tp, fp, fn = evaluate_detections(pred_boxes, gt_boxes, iou_threshold)\n                TP += tp\n                FP += fp\n                FN += fn\n\n    precision, recall, f1_score = calculate_metrics(TP, FP, FN)\n    accuracy = calculate_accuracy(TP, FP, FN)\n    \n    return precision, recall, f1_score, accuracy\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-20T22:47:25.124240Z","iopub.execute_input":"2024-05-20T22:47:25.124590Z","iopub.status.idle":"2024-05-20T22:47:25.141933Z","shell.execute_reply.started":"2024-05-20T22:47:25.124561Z","shell.execute_reply":"2024-05-20T22:47:25.141108Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\nprecision, recall, f1_score, accuracy = evaluate_model(model, train_loader, device)\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1_score:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T22:47:26.752751Z","iopub.execute_input":"2024-05-20T22:47:26.753116Z","iopub.status.idle":"2024-05-20T22:47:29.429934Z","shell.execute_reply.started":"2024-05-20T22:47:26.753091Z","shell.execute_reply":"2024-05-20T22:47:29.429035Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Precision: 0.6790\nRecall: 0.9649\nF1 Score: 0.7971\nAccuracy: 0.6627\n","output_type":"stream"}]}]}